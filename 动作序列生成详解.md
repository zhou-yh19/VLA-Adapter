# 动作序列生成机制详解

对于您的场景（30个动作，每个16维）：

**用L1回归方式（使用action_head）**
- 模型只进行一次forward pass，得到hidden states
- action_head直接从hidden states预测连续动作
- 输出形状：`(30, 16)` - 直接是连续动作值
- **关键**：使用**多层hidden states**（multi_layer_hidden_states），而不是KV_cache

---

## 详细机制解析

### 一、常量定义（Teleavatar）

```python
# 来自 prismatic/vla/constants.py
NUM_ACTIONS_CHUNK = 30    # 每个chunk包含30个动作
ACTION_DIM = 16           # 每个动作16维
NUM_TOKENS = 64           # 注意：这个值可能不是480！
```

**说明**：`NUM_TOKENS = 64` 用于action queries的数量，用于action_head的输入。

---

### 二、L1回归方式（使用action_head）

#### 2.1 生成流程

```python
if action_head is not None:
    # 1. 提取hidden states（不是logits）
    actions_hidden_states = text_hidden_states[
        :, 
        NUM_PATCHES + NUM_PROMPT_TOKENS : 
        NUM_PATCHES + NUM_PROMPT_TOKENS + NUM_TOKENS, 
        :
    ]
    # 形状：(1, NUM_TOKENS, hidden_dim)，例如 (1, 64, 4096)
    
    # 2. 通过action_head预测连续动作
    normalized_actions = action_head.predict_action(
        multi_layer_hidden_states,
        proprio=proprio,  # 机器人状态（本体感觉信息）
        proprio_projector=proprio_projector  # 投影层，将proprio投影到LLM空间
    )
    # 输出形状：(NUM_ACTIONS_CHUNK, ACTION_DIM) = (30, 16)
```

**关键点**：
- **不需要生成480个token**
- 只需要一次forward pass，得到hidden states
- action_head直接从hidden states预测连续动作值
- **为什么用hidden states而不是KV_cache？** 见下文详细解释

---

### 三、架构图中的"KV"与代码实现的关系

#### 3.1 重要澄清：两种不同的"KV"概念

**Q: 论文架构图显示VLM的KV被输入到action head的Bridge Attention，但代码中为什么说不用KV_cache？**

**A: 这里需要区分两个概念：**

1. **架构图中的"KV"**：指的是从VLM hidden states中**动态提取并投影**得到的key-value表示，用于Bridge Attention的cross-attention机制
2. **传统意义上的"kv-cache"**：指的是用于**自回归生成**时缓存之前计算的key-value，避免重复计算

**VLA-Adapter使用的是前者，不是后者！**

#### 3.2 架构图中的"KV"如何实现

**Bridge Attention中的KV生成过程**：

```python
# action_heads.py MLPResNetBlock_Pro.forward()
# 1. 接收VLM的hidden states（多层）
h_t = task_hidden_states  # 形状: (batch, num_layers, num_task_tokens, hidden_dim)
h_a = actions_hidden_states  # 形状: (batch, num_layers, num_action_tokens, hidden_dim)

# 2. 在每一层Bridge Attention中，从对应层的hidden states生成KV
# MLPResNet.forward() 第118行
for i, block in enumerate(self.mlp_resnet_blocks):
    x = block(x, 
              h_t = h_t[:,i+1,:],  # 使用VLM第i+1层的task hidden states
              h_a = h_a[:,i+1,:],  # 使用VLM第i+1层的action hidden states
              p=p)

# 3. 在Bridge Attention中，将hidden states投影成KV
# MLPResNetBlock_Pro.forward() 第362-367行
k_adapter = self.k_adapter(h_adapter)  # 从action hidden states生成key
v_adapter = self.v_adapter(h_adapter)  # 从action hidden states生成value
k_task = self.k_task(h_task)          # 从task hidden states生成key
v_task = self.v_task(h_task)          # 从task hidden states生成value

# 4. 使用这些KV进行cross-attention
attn_scores_adapter = torch.matmul(q_1, k_adapter.transpose(-2, -1))
attn_scores_task = torch.matmul(q_1, k_task.transpose(-2, -1))
```

**关键点**：
- ✅ **架构图中的"KV"确实被输入到action head**：每一层Bridge Attention都接收VLM对应层的hidden states，并从中生成key-value
- ✅ **多层传递**：VLM的每一层hidden states都被传递到action head的对应层Bridge Attention
- ❌ **但不是传统意义上的kv-cache**：这些KV是从hidden states动态生成的，不是预先缓存的

#### 3.3 为什么不用传统意义上的kv-cache？

**传统kv-cache的用途**：
- 主要用于**自回归生成**（autoregressive generation）
- 每次只生成一个token，缓存之前计算的key-value以避免重复计算
- 适用于需要循环多次forward pass的场景

**VLA模型的实际情况**：
```python
# modeling_prismatic.py 第834-845行
language_model_output = self.language_model(
    input_ids=None,
    attention_mask=multimodal_attention_mask,
    position_ids=None,
    past_key_values=None,  # ❌ 明确设置为None！
    inputs_embeds=multimodal_embeddings,
    labels=None,
    use_cache=None,  # ❌ 不使用传统kv-cache！
    output_hidden_states=True,  # ✅ 需要hidden states
    ...
)
```

**关键点**：
- VLA使用**并行生成**（一次forward pass），不是自回归生成
- 所有动作位置的hidden states**同时计算**，不需要缓存
- 但Bridge Attention需要从这些hidden states中**动态生成KV**用于cross-attention

#### 3.4 多层传递机制（对应架构图中的多层KV）

**action_head需要多层hidden states**：

```python
# finetune.py 第396-409行
multi_layer_hidden_states = []

# 收集所有层的hidden states（不仅仅是最后一层！）
for item in output.hidden_states[0:]:  # 遍历所有层
    actions_hidden_states = text_hidden_states[current_action_mask | next_actions_mask]
    task_latten_states = item[:, :num_patches]  # 视觉patch的hidden states
    all_hidden_states = torch.cat((task_latten_states, actions_hidden_states), 2)
    multi_layer_hidden_states.append(all_hidden_states)

multi_layer_hidden_states = torch.cat(multi_layer_hidden_states, dim=1)
# 形状：(batch, num_layers, num_tokens, hidden_dim)
```

**在action_head中的多层使用**：

```python
# action_heads.py MLPResNet.forward() 第118行
for i, block in enumerate(self.mlp_resnet_blocks):
    # 每一层Bridge Attention接收VLM对应层的hidden states
    x = block(x, 
              h_t = h_t[:,i+1,:],  # VLM第i+1层的task hidden states → 生成k_task, v_task
              h_a = h_a[:,i+1,:],  # VLM第i+1层的action hidden states → 生成k_adapter, v_adapter
              p=p)
```

**为什么需要多层？**
- 不同层捕获不同级别的特征（底层→高层：细节→抽象）
- 每一层Bridge Attention需要对应层的KV信息，实现**层级对齐**的cross-attention
- 这正好对应架构图中VLM的每一层KV都连接到Policy对应层的Bridge Attention

#### 3.5 需要完整的序列信息

**action_head需要的信息**：

```python
# action_heads.py 第57-78行
def predict_action(self, actions_hidden_states, ...):
    # 分离不同部分的信息
    task_hidden_states = actions_hidden_states[:, :, :self.num_task_tokens, :]  # 视觉patch
    actions_hidden_states = actions_hidden_states[:, :, self.num_task_tokens:, :]  # 动作query
    
    # action_head需要：
    # 1. 视觉信息（task_hidden_states）
    # 2. 动作query信息（actions_hidden_states）
    # 3. 多层表示（multi_layer_hidden_states）
    # 4. 机器人状态（proprio_features）
    action = self.model(
        rearranged_actions_hidden_states,
        h_a=actions_hidden_states,  # 动作hidden states
        p=proprio_features,  # 本体感觉特征（机器人状态）
        h_t=task_hidden_states  # 任务（视觉）hidden states
    )
```

**KV_cache的限制**：
- KV_cache只包含attention机制的key-value对
- **不包含**：
  - 完整的hidden states表示
  - 视觉patch的信息
  - 多层级的特征
- 因此**不适合**action_head的需求

#### 3.6 训练和推理的一致性

**训练时**：
```python
# 训练时使用完整的forward pass，得到所有层的hidden states
output = vla(..., output_hidden_states=True)
multi_layer_hidden_states = output.hidden_states  # 所有层
```

**推理时**：
```python
# 推理时也使用相同的机制，保持一致性
language_model_output = self.language_model(..., output_hidden_states=True)
multi_layer_hidden_states = language_model_output.hidden_states  # 所有层
```

**关键**：训练和推理使用相同的机制，确保一致性。如果训练时用hidden states，推理时用KV_cache，会导致不一致。

#### 3.7 总结对比

| 特性 | Hidden States → KV（架构图中的KV） | 传统kv-cache |
|------|--------------|----------|
| **来源** | 从VLM hidden states动态投影生成 | 预先计算并缓存的key-value |
| **生成方式** | 在Bridge Attention中通过`k_proj`/`v_proj`投影 | 在自回归生成时缓存 |
| **包含信息** | ✅ 完整的hidden states表示<br>✅ 视觉patch信息<br>✅ 动作query信息<br>✅ 多层特征 | ❌ 只有预计算的key-value<br>❌ 不包含完整表示 |
| **传递方式** | ✅ 多层传递：VLM每一层 → Policy对应层Bridge Attention | ❌ 单层缓存 |
| **适用场景** | 并行生成、需要多层cross-attention | 自回归生成、加速推理 |
| **VLA使用** | ✅ 使用（架构图中的KV） | ❌ 不使用（`past_key_values=None`） |
| **action_head需求** | ✅ 完全满足（Bridge Attention需要） | ❌ 无法满足 |

**关键理解**：
- ✅ **架构图中的"KV"确实存在**：VLM的每一层hidden states都被传递到action head，在Bridge Attention中生成KV用于cross-attention
- ✅ **多层对齐**：VLM的第i层hidden states → Policy的第i层Bridge Attention
- ❌ **但不是传统kv-cache**：这些KV是从hidden states动态生成的，不是预先缓存的

---

### 四、关于Hidden States vs KV_cache的结论

**Q: 为什么LLM输入给动作头的是hidden states而不是KV_cache？**

**A: 核心原因总结：**

1. ✅ **并行生成不需要KV_cache**：VLA使用并行生成（一次forward pass），不是自回归生成，因此不需要缓存
2. ✅ **需要多层信息**：action_head需要所有层的hidden states，而不仅仅是最后一层
3. ✅ **需要完整表示**：hidden states包含视觉patch、动作query等完整信息，KV_cache只包含key-value
4. ✅ **训练推理一致性**：训练和推理使用相同的机制，确保一致性

**代码证据**：
```python
# 明确不使用KV_cache
past_key_values=None,
use_cache=None,
output_hidden_states=True,  # 需要hidden states
```

---

## 总结

### VLA-Adapter的关键特点

1. **使用L1回归方式**：
   - 使用 `action_head` 直接从hidden states预测连续动作值
   - 不需要生成离散token，输出形状为 `(30, 16)`

2. **多层hidden states传递**：
   - VLM的所有层hidden states都被传递到action head
   - 每一层Bridge Attention接收对应层的hidden states，并从中生成KV用于cross-attention

3. **并行生成**：
   - 使用一次forward pass，不是自回归生成
   - 所有动作位置的hidden states同时计算

4. **Initial Action**：
   - 推理时是全零张量
   - 训练时会添加可学习的随机扰动

---

## 五、机器人状态（Proprio）的传入位置和使用方式

### 5.1 重要澄清：Proprio不在VLM中处理

**关键理解**：proprio**不会**输入到VLM中生成hidden states，而是**只在Action Head中使用**。

**错误理解**：
- ❌ proprio通过projector输入到VLM中生成多个隐藏层向量
- ❌ 这些向量会被放到后面的action head中生成动作

**正确理解**：
- ✅ proprio**不参与**VLM的forward pass
- ✅ VLM只处理：Vision Patches + Text Tokens + ActionQuery
- ✅ proprio**只在Action Head阶段**传入，通过`proprio_projector`投影后参与Bridge Attention

### 5.2 传入位置

机器人状态（proprioceptive state，简称proprio）在**Action Head的`predict_action`方法**中传入，具体位置如下：

```python
# action_heads.py L1RegressionActionHead.predict_action() 第43-48行
def predict_action(
        self, 
        actions_hidden_states, 
        proprio=None,  # ← 机器人状态在这里传入
        proprio_projector=None,  # ← 投影层
        phase="Inference"
        ):
```

**调用链**：
1. **微调/推理时**：`vla.predict_action()` → `action_head.predict_action()` → `MLPResNet.forward()` → `MLPResNetBlock_Pro.forward()`
2. **训练时**：`vla()` → `_regression_or_discrete_prediction()` → `action_head.predict_action()`

### 5.3 为什么Proprio不在VLM中？

**代码证据**：

1. **VLM的forward pass中不包含proprio**：
```python
# prismatic/extern/hf/modeling_prismatic.py _regression_or_discrete_prediction() 第828-831行
# Build multimodal embeddings and attention mask
multimodal_embeddings, multimodal_attention_mask = self._build_multimodal_attention(
    input_embeddings, projected_patch_embeddings, attention_mask
)
# 注意：这里没有proprio！
```

2. **Multimodal embeddings的组成**：
```python
# prismatic/extern/hf/modeling_prismatic.py _build_multimodal_attention() 第500-502行
multimodal_embeddings = torch.cat(
    [input_embeddings[:, :1, :], projected_patch_embeddings, input_embeddings[:, 1:, :]], dim=1
)
# 组成：[BOS] + [Vision Patches] + [Text + ActionQuery]
# 注意：不包含proprio！
```

3. **虽然存在`_process_proprio_features`方法，但实际未使用**：
```python
# prismatic/extern/hf/modeling_prismatic.py 第474-484行
def _process_proprio_features(self, projected_patch_embeddings, proprio, proprio_projector):
    """Process proprioceptive features and append to vision features"""
    # 这个方法存在，但在VLA-Adapter的实际forward pass中**没有被调用**
```

**设计原因**：
- VLM专注于视觉-语言理解，不需要机器人状态
- Action Head负责动作预测，需要机器人当前状态来生成连续动作
- 分离关注点：VLM理解任务，Action Head执行动作

### 5.4 处理流程

#### Step 1: 投影到LLM空间（在Action Head中）

```python
# action_heads.py 第53-55行
proprio = proprio.reshape(batch_size, -1).to(torch.bfloat16)  # (bsz, proprio_dim)
proprio_features = proprio_projector(proprio)  # (bsz, llm_dim)
proprio_features = proprio_features.unsqueeze(dim=1)  # (bsz, 1, llm_dim)
```

**说明**：
- `proprio`：原始机器人状态，形状为`(batch_size, proprio_dim)`，例如`(1, 14)`（左臂7维 + 右臂7维）
- `proprio_projector`：线性投影层，将proprio从`proprio_dim`投影到`llm_dim`（例如：14 → 4096）
- `proprio_features`：投影后的特征，形状为`(batch_size, 1, llm_dim)`

#### Step 2: 传入MLPResNet

```python
# action_heads.py 第74-79行
action = self.model(
    rearranged_actions_hidden_states,
    h_a=actions_hidden_states,  # 动作hidden states
    p=proprio_features,  # ← 机器人状态特征
    h_t=task_hidden_states  # 任务（视觉）hidden states
)
```

#### Step 3: 在Bridge Attention中使用

在`MLPResNetBlock_Pro`的每一层中，proprio与action hidden states拼接，用于生成KV：

```python
# action_heads.py MLPResNetBlock_Pro.forward() 第346-347行
# concat h_a and p
h_adapter = torch.cat((h_a, p), dim=1)  # 拼接action hidden states和proprio

# 第362-363行：从h_adapter生成KV
k_adapter = self.k_adapter(h_adapter)  # 生成key
v_adapter = self.v_adapter(h_adapter)  # 生成value
```

**关键点**：
- `h_a`：action hidden states，形状为`(batch_size, num_action_tokens, llm_dim)`，例如`(1, 64, 4096)`
- `p`：proprio features，形状为`(batch_size, 1, llm_dim)`，例如`(1, 1, 4096)`
- `h_adapter`：拼接后的adapter tokens，形状为`(batch_size, num_action_tokens + 1, llm_dim)`，例如`(1, 65, 4096)`

#### Step 4: 参与Cross-Attention

```python
# action_heads.py 第389-391行
attn_scores = [torch.matmul(q_1, k_tokens.transpose(-2, -1))]  # self-attention
attn_scores.append(torch.matmul(q_1, k_adapter.transpose(-2, -1)))  # ← 包含proprio的cross-attention
attn_scores.append(torch.matmul(q_1, k_task.transpose(-2, -1)) * ratio_g)  # task cross-attention
```

**说明**：
- Query（`q_1`）来自action head的内部表示（`x`）
- Key-Value（`k_adapter`, `v_adapter`）来自拼接后的`h_adapter`（包含action hidden states和proprio）
- 通过cross-attention，action head可以**同时关注**：
  1. Action query的hidden states（`h_a`）
  2. 机器人当前状态（`p`）
  3. 视觉任务信息（`h_t`）

### 5.5 完整数据流

```
机器人状态（proprio）
    ↓
   形状：(batch_size, proprio_dim)，例如 (1, 14)
    ↓
proprio_projector（线性投影）
    ↓
   形状：(batch_size, llm_dim)，例如 (1, 4096)
    ↓
unsqueeze(dim=1)
    ↓
   形状：(batch_size, 1, llm_dim)，例如 (1, 1, 4096)
    ↓
传入MLPResNet.forward(p=proprio_features)
    ↓
在每一层MLPResNetBlock_Pro中：
    h_adapter = concat(h_a, p)
    ↓
    形状：(batch_size, num_action_tokens + 1, llm_dim)，例如 (1, 65, 4096)
    ↓
    生成KV：k_adapter, v_adapter = k_adapter(h_adapter), v_adapter(h_adapter)
    ↓
    参与cross-attention：q_1 @ k_adapter^T
    ↓
    输出：融合了action query和proprio信息的特征
```

### 5.6 为什么需要Proprio？

1. **状态感知**：机器人需要知道当前的关节角度、位置等状态，才能预测下一步动作
2. **动作连续性**：proprio提供了当前状态信息，帮助模型生成连续、平滑的动作序列
3. **多模态融合**：proprio与视觉、文本信息一起，形成完整的场景理解

### 5.7 代码位置总结

| 组件 | 文件位置 | 关键代码行 |
|------|---------|-----------|
| **传入入口** | `prismatic/extern/hf/modeling_prismatic.py` | `predict_action()` 第896行：`proprio=None` |
| **投影处理** | `prismatic/models/action_heads.py` | `L1RegressionActionHead.predict_action()` 第53-55行 |
| **传入MLPResNet** | `prismatic/models/action_heads.py` | `MLPResNet.forward()` 第111行：`p=None` |
| **Bridge Attention使用** | `prismatic/models/action_heads.py` | `MLPResNetBlock_Pro.forward()` 第347行：`h_adapter = torch.cat((h_a, p), dim=1)` |
| **训练时传入** | `vla-scripts/finetune.py` | `run_forward_pass()` 第342行：`proprio=batch["proprio"] if use_proprio else None` |
| **推理时传入** | `experiments/robot/openvla_utils.py` | `get_vla_action()` 第835行：`proprio=proprio` |

### 5.8 关键理解

1. **传入时机**：proprio**只在**Action Head阶段传入，**完全不参与**VLM的forward pass
2. **VLM的输入**：VLM只处理Vision Patches + Text Tokens + ActionQuery，**不包含proprio**
3. **投影方式**：通过`proprio_projector`（线性层）投影到LLM的embedding空间
4. **融合方式**：与action hidden states拼接，一起参与Bridge Attention的cross-attention
5. **多层传递**：proprio特征在每一层Bridge Attention中都参与cross-attention，实现层级对齐
6. **可选性**：如果`use_proprio=False`或`proprio=None`，模型仍可运行，但可能影响性能
7. **架构分离**：VLM负责理解（视觉+语言），Action Head负责执行（动作+状态）
